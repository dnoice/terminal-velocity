#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚úí Metadata
    - Title: Terminal Velocity Scaffold Builder (TV-Framework Edition - v1.0)
    - File Name: tv_scaffold.py
    - Relative Path: tools/tv_scaffold.py
    - Artifact Type: CLI
    - Version: 1.0.0
    - Date: 2025-12-14
    - Update: Sunday, December 14, 2025
    - Author: Dennis 'dnoice' Smaltz
    - A.I. Acknowledgement: Anthropic - Claude Opus 4.5
    - Signature:  Ô∏ª„Éá‚ïê‚Äî¬∑¬∑¬∑ üéØ = Aim Twice, Shoot Once!

‚úí Description:
    Constructs the complete Terminal Velocity research framework directory structure
    from scratch. Generates all folders, template files, mechanism dossier scaffolds,
    notebook templates, and configuration files according to the TV Blueprint spec.
    Supports partial builds, custom mechanism definitions, and dry-run previews.

‚úí Key Features:
    - Feature 1: Full directory tree generation matching TV Blueprint architecture
    - Feature 2: Template generation for all document types (DOSSIER.md, notebooks, etc.)
    - Feature 3: Mechanism inventory initialization with all 50+ default mechanisms
    - Feature 4: Custom mechanism YAML import for extending the inventory
    - Feature 5: Dry-run mode with tree visualization before execution
    - Feature 6: Selective vector building (build only vectors you need)
    - Feature 7: Notebook template generation with environment.yml
    - Feature 8: Git initialization with .gitignore and .gitattributes
    - Feature 9: Progress indicators with rich terminal output
    - Feature 10: Idempotent operation (safe to run multiple times)
    - Feature 11: Manifest generation tracking all created files
    - Feature 12: Configurable output via YAML config file

‚úí Usage Instructions:
    Run from command line to scaffold a new Terminal Velocity project:
    
    Basic usage (creates full structure in current directory):
        $ python tv_scaffold.py ./my-research-project
    
    Preview what will be created:
        $ python tv_scaffold.py ./my-project --dry-run
    
    Build only specific vectors:
        $ python tv_scaffold.py ./my-project --vectors A B C
    
    Import custom mechanisms from YAML:
        $ python tv_scaffold.py ./my-project --mechanisms custom_mechanisms.yaml

‚úí Examples:
    # Full build with all defaults
    $ python tv_scaffold.py ~/research/terminal-velocity
    
    # Dry run to preview structure
    $ python tv_scaffold.py ~/research/terminal-velocity --dry-run
    
    # Build only atmospheric and biosphere vectors
    $ python tv_scaffold.py ~/research/tv-subset --vectors A B
    
    # Verbose output with progress details
    $ python tv_scaffold.py ./tv-project --verbose
    
    # Skip git initialization
    $ python tv_scaffold.py ./tv-project --no-git
    
    # Custom mechanisms from YAML file
    $ python tv_scaffold.py ./tv-project --mechanisms my_mechanisms.yaml
    
    # Force overwrite existing files (use with caution)
    $ python tv_scaffold.py ./tv-project --force
    
    # Generate only infrastructure (no mechanism dossiers)
    $ python tv_scaffold.py ./tv-project --infrastructure-only
    
    # Quiet mode (errors only)
    $ python tv_scaffold.py ./tv-project --quiet

‚úí Command-Line Arguments:
    Positional Arguments:
        target_dir               Root directory for the project (will be created)
    
    Vector Selection:
        --vectors, -V            Space-separated list of vectors to build (A B C D E F)
                                 Default: all vectors
        --infrastructure-only    Build only shared infrastructure, no dossiers
    
    Mechanism Options:
        --mechanisms FILE        YAML file with custom mechanism definitions
        --skip-defaults          Don't include default mechanisms (use with --mechanisms)
    
    Output Control:
        --dry-run, -n            Preview directory tree without creating anything
        --force, -f              Overwrite existing files (default: skip existing)
        --verbose, -v            Detailed progress output
        --quiet, -q              Suppress all output except errors
    
    Git Options:
        --no-git                 Skip git repository initialization
        --git-remote URL         Set git remote origin after init
    
    Configuration:
        --config FILE            Load settings from YAML config file
        --save-config FILE       Save current settings to YAML config file

‚úí Other Important Information:
    - Dependencies: 
        Required: pathlib, argparse, textwrap, datetime (stdlib)
        Optional: rich (enhanced terminal UI), pyyaml (config files)
    - Compatible platforms: Linux, macOS, Windows, Termux
    - Performance notes: Creates ~500 files/folders in <5 seconds
    - Security considerations: Validates paths to prevent directory traversal
    - Known limitations: 
        * Cannot merge with existing non-TV directory structures
        * Custom mechanism YAML requires specific schema (see docs)
    - Exit codes:
        0 = Success
        1 = Invalid arguments
        2 = Target exists and --force not specified
        3 = Permission denied
        4 = Invalid mechanism YAML
---------
"""

from __future__ import annotations

import argparse
import os
import sys
import textwrap
from datetime import datetime
from pathlib import Path
from typing import Any

# Optional imports with fallbacks
try:
    from rich.console import Console
    from rich.tree import Tree
    from rich.progress import Progress, SpinnerColumn, TextColumn
    from rich.panel import Panel
    from rich import print as rprint
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False


# =============================================================================
# CONSTANTS & CONFIGURATION
# =============================================================================

VERSION = "1.0.0"
PROGRAM_NAME = "tv_scaffold"
SIGNATURE = "Ô∏ª„Éá‚ïê‚Äî¬∑¬∑¬∑ üéØ = Aim Twice, Shoot Once!"

# Default mechanism inventory by vector
DEFAULT_MECHANISMS: dict[str, list[dict[str, str]]] = {
    "A": [  # Atmospheric Loading
        {"code": "A-01", "slug": "coal_combustion", "name": "Coal Combustion Chain", "status": "Active", "confidence": 5},
        {"code": "A-02", "slug": "fugitive_methane", "name": "Fugitive Methane Systems", "status": "Active", "confidence": 4},
        {"code": "A-03", "slug": "cement_emissions", "name": "Cement Process Emissions", "status": "Active", "confidence": 5},
        {"code": "A-04", "slug": "refrigerant_leakage", "name": "Refrigerant Leakage Cycle", "status": "Active", "confidence": 4},
        {"code": "A-05", "slug": "biomass_accounting", "name": "Biomass Accounting Fraud", "status": "Active", "confidence": 3},
        {"code": "A-06", "slug": "aviation_forcing", "name": "Aviation Radiative Forcing", "status": "Active", "confidence": 4},
        {"code": "A-07", "slug": "shipping_emissions", "name": "Shipping Heavy Fuel Emissions", "status": "Active", "confidence": 4},
        {"code": "A-08", "slug": "agricultural_n2o", "name": "Agricultural N‚ÇÇO Release", "status": "Active", "confidence": 4},
        {"code": "A-09", "slug": "permafrost_feedback", "name": "Permafrost Feedback Loop", "status": "Accelerating", "confidence": 3},
        {"code": "A-10", "slug": "datacenter_load", "name": "Data Center Grid Load", "status": "Accelerating", "confidence": 4},
    ],
    "B": [  # Biosphere Erasure
        {"code": "B-01", "slug": "agricultural_conversion", "name": "Agricultural Conversion Engine", "status": "Active", "confidence": 5},
        {"code": "B-02", "slug": "deforestation_chains", "name": "Deforestation Supply Chains", "status": "Active", "confidence": 4},
        {"code": "B-03", "slug": "fishing_collapse", "name": "Industrial Fishing Collapse", "status": "Active", "confidence": 4},
        {"code": "B-04", "slug": "fragmentation", "name": "Fragmentation by Infrastructure", "status": "Active", "confidence": 5},
        {"code": "B-05", "slug": "invasive_trade", "name": "Invasive Species Trade Routes", "status": "Active", "confidence": 3},
        {"code": "B-06", "slug": "pesticide_cascade", "name": "Pesticide Pollinator Cascade", "status": "Active", "confidence": 4},
        {"code": "B-07", "slug": "coral_thermal", "name": "Coral Thermal Threshold", "status": "Critical", "confidence": 5},
        {"code": "B-08", "slug": "wetland_drainage", "name": "Wetland Drainage Economics", "status": "Active", "confidence": 4},
        {"code": "B-09", "slug": "wildlife_trade", "name": "Wildlife Trade Networks", "status": "Active", "confidence": 3},
        {"code": "B-10", "slug": "paper_parks", "name": "Paper Parks & Enforcement Gaps", "status": "Active", "confidence": 4},
    ],
    "C": [  # Hydrological Disruption
        {"code": "C-01", "slug": "aquifer_mining", "name": "Aquifer Mining Operations", "status": "Active", "confidence": 4},
        {"code": "C-02", "slug": "irrigation_paradox", "name": "Irrigation Efficiency Paradox", "status": "Active", "confidence": 4},
        {"code": "C-03", "slug": "river_regulation", "name": "River Regulation Syndrome", "status": "Active", "confidence": 5},
        {"code": "C-04", "slug": "industrial_water", "name": "Industrial Water Appropriation", "status": "Active", "confidence": 4},
        {"code": "C-05", "slug": "nutrient_deadzones", "name": "Nutrient Loading Dead Zones", "status": "Active", "confidence": 5},
        {"code": "C-06", "slug": "urban_decay", "name": "Urban Infrastructure Decay", "status": "Active", "confidence": 4},
        {"code": "C-07", "slug": "desalination_brine", "name": "Desalination Brine Discharge", "status": "Growing", "confidence": 3},
        {"code": "C-08", "slug": "glacial_loss", "name": "Glacial Loss Acceleration", "status": "Critical", "confidence": 5},
    ],
    "D": [  # Chemical Contamination
        {"code": "D-01", "slug": "pfas_pathways", "name": "PFAS Proliferation Pathways", "status": "Active", "confidence": 4},
        {"code": "D-02", "slug": "microplastic_saturation", "name": "Microplastic Saturation", "status": "Active", "confidence": 3},
        {"code": "D-03", "slug": "pm25_exposure", "name": "PM2.5 Chronic Exposure", "status": "Active", "confidence": 5},
        {"code": "D-04", "slug": "ewaste_processing", "name": "E-Waste Informal Processing", "status": "Active", "confidence": 4},
        {"code": "D-05", "slug": "agricultural_runoff", "name": "Agricultural Chemical Runoff", "status": "Active", "confidence": 5},
        {"code": "D-06", "slug": "pharmaceutical_contamination", "name": "Pharmaceutical Water Contamination", "status": "Growing", "confidence": 3},
        {"code": "D-07", "slug": "heavy_metal_legacy", "name": "Heavy Metal Legacy Sites", "status": "Active", "confidence": 5},
        {"code": "D-08", "slug": "endocrine_disruptors", "name": "Endocrine Disruptor Dispersion", "status": "Active", "confidence": 3},
    ],
    "E": [  # Resource Depletion
        {"code": "E-01", "slug": "fossil_extraction", "name": "Fossil Fuel Extraction Acceleration", "status": "Active", "confidence": 5},
        {"code": "E-02", "slug": "critical_mineral_race", "name": "Critical Mineral Race", "status": "Accelerating", "confidence": 4},
        {"code": "E-03", "slug": "soil_loss", "name": "Soil Carbon/Fertility Loss", "status": "Active", "confidence": 4},
        {"code": "E-04", "slug": "phosphorus_peak", "name": "Phosphorus Peak Trajectory", "status": "Active", "confidence": 4},
        {"code": "E-05", "slug": "sand_mining", "name": "Sand Mining Crisis", "status": "Active", "confidence": 3},
        {"code": "E-06", "slug": "rare_earth", "name": "Rare Earth Concentration", "status": "Active", "confidence": 4},
    ],
    "F": [  # Systemic Lock-in
        {"code": "F-01", "slug": "fossil_subsidies", "name": "Fossil Fuel Subsidy Architecture", "status": "Active", "confidence": 5},
        {"code": "F-02", "slug": "externality_evasion", "name": "Externality Accounting Evasion", "status": "Active", "confidence": 5},
        {"code": "F-03", "slug": "regulatory_capture", "name": "Regulatory Capture Dynamics", "status": "Active", "confidence": 4},
        {"code": "F-04", "slug": "growth_imperative", "name": "Growth Imperative Mechanics", "status": "Active", "confidence": 4},
        {"code": "F-05", "slug": "supply_chain_opacity", "name": "Supply Chain Opacity Games", "status": "Active", "confidence": 4},
        {"code": "F-06", "slug": "greenwash_failure", "name": "Greenwash Verification Failure", "status": "Active", "confidence": 5},
        {"code": "F-07", "slug": "adaptation_inequity", "name": "Climate Adaptation Inequity", "status": "Active", "confidence": 4},
        {"code": "F-08", "slug": "environmental_injustice", "name": "Environmental Injustice Gradients", "status": "Active", "confidence": 5},
        {"code": "F-09", "slug": "emissions_leakage", "name": "International Emissions Leakage", "status": "Active", "confidence": 4},
        {"code": "F-10", "slug": "stranded_assets", "name": "Stranded Asset Political Economy", "status": "Active", "confidence": 4},
    ],
}

VECTOR_NAMES: dict[str, str] = {
    "A": "atmospheric",
    "B": "biosphere",
    "C": "hydrological",
    "D": "chemical",
    "E": "depletion",
    "F": "systemic",
}

VECTOR_FULL_NAMES: dict[str, str] = {
    "A": "Atmospheric Loading",
    "B": "Biosphere Erasure",
    "C": "Hydrological Disruption",
    "D": "Chemical Contamination",
    "E": "Resource Depletion",
    "F": "Systemic Lock-in",
}


# =============================================================================
# TEMPLATE CONTENT
# =============================================================================

def get_readme_template() -> str:
    """Generate the main README.md template."""
    return textwrap.dedent("""\
        # TERMINAL VELOCITY
        
        ## A Forensic Investigation into Civilizational Self-Destruction
        
        ---
        
        > *"Terminal velocity is the maximum speed attainable by an object as it falls 
        > through a fluid. We are the object. The fluid is time. The ground is non-negotiable."*
        
        ---
        
        ## Overview
        
        This repository contains the complete research framework for documenting mechanisms
        by which industrial civilization accelerates toward hard planetary boundaries.
        
        ## Structure
        
        ```
        terminal-velocity/
        ‚îú‚îÄ‚îÄ mechanism_dossiers/     # Primary research units by damage vector
        ‚îú‚îÄ‚îÄ connection_atlas/       # Interconnection mapping
        ‚îú‚îÄ‚îÄ evidence_vault/         # Datasets and source materials
        ‚îú‚îÄ‚îÄ synthesis_products/     # Derived outputs
        ‚îú‚îÄ‚îÄ computational_core/     # Shared code and utilities
        ‚îú‚îÄ‚îÄ visual_assets/          # Templates and iconography
        ‚îî‚îÄ‚îÄ meta/                   # Project metadata
        ```
        
        ## Quick Start
        
        1. Navigate to a mechanism dossier in `mechanism_dossiers/`
        2. Read the `DOSSIER.md` for the executive summary
        3. Explore `evidence_archive/` for underlying data
        4. Run notebooks in `evidence_archive/computations/` to reproduce findings
        
        ## Vectors
        
        | Vector | Focus | Mechanisms |
        |--------|-------|------------|
        | A | Atmospheric Loading | {a_count} |
        | B | Biosphere Erasure | {b_count} |
        | C | Hydrological Disruption | {c_count} |
        | D | Chemical Contamination | {d_count} |
        | E | Resource Depletion | {e_count} |
        | F | Systemic Lock-in | {f_count} |
        
        ## Philosophy
        
        We are not advocates. We are not activists. We are coroners.
        
        **Filed under: Causes of Death, Preventable**
        
        ---
        
        *Generated: {date}*  
        *{signature}*
        """.format(
            a_count=len(DEFAULT_MECHANISMS.get("A", [])),
            b_count=len(DEFAULT_MECHANISMS.get("B", [])),
            c_count=len(DEFAULT_MECHANISMS.get("C", [])),
            d_count=len(DEFAULT_MECHANISMS.get("D", [])),
            e_count=len(DEFAULT_MECHANISMS.get("E", [])),
            f_count=len(DEFAULT_MECHANISMS.get("F", [])),
            date=datetime.now().strftime("%Y-%m-%d"),
            signature=SIGNATURE,
        ))


def get_manifest_template(mechanisms: dict[str, list]) -> str:
    """Generate the MANIFEST.md tracking all mechanisms."""
    lines = [
        "# MECHANISM INVENTORY",
        "",
        "Current status of all mechanism dossiers.",
        "",
        "---",
        "",
    ]
    
    for vector_code in sorted(mechanisms.keys()):
        vector_name = VECTOR_FULL_NAMES.get(vector_code, vector_code)
        lines.append(f"## Vector {vector_code}: {vector_name}")
        lines.append("")
        lines.append("| Code | Mechanism | Status | Confidence | Progress |")
        lines.append("|------|-----------|--------|------------|----------|")
        
        for mech in mechanisms[vector_code]:
            stars = "‚òÖ" * mech.get("confidence", 3) + "‚òÜ" * (5 - mech.get("confidence", 3))
            lines.append(
                f"| {mech['code']} | {mech['name']} | {mech.get('status', 'Active')} | {stars} | ‚¨ú Draft |"
            )
        
        lines.append("")
    
    lines.extend([
        "---",
        "",
        "### Progress Key",
        "",
        "- ‚¨ú Draft - Dossier template created",
        "- üü® Evidence - Data gathering in progress",
        "- üü¶ Analysis - Computations underway",
        "- üü© Review - Internal review",
        "- ‚úÖ Published - Complete and verified",
        "",
        f"*Last updated: {datetime.now().strftime('%Y-%m-%d')}*",
    ])
    
    return "\n".join(lines)


def get_methodology_template() -> str:
    """Generate METHODOLOGY.md with research standards."""
    return textwrap.dedent("""\
        # METHODOLOGY
        
        ## Terminal Velocity Research Standards
        
        ---
        
        ## Evidence Grading System
        
        | Grade | Name | Definition |
        |-------|------|------------|
        | **I** | Direct Measurement | Empirical observations from peer-reviewed monitoring |
        | **II** | Official Inventory | Government/multilateral agency accounting |
        | **III** | Validated Model | Peer-reviewed models with documented assumptions |
        | **IV** | Expert Assessment | Consensus statements, systematic reviews |
        | **V** | Qualified Report | Industry reporting, investigative journalism |
        | **VI** | Contextual | Commentary (background only, never as proof) |
        
        ## Usage Rules
        
        1. **Damage claims require Grade I-II evidence.** No exceptions.
        2. **Causal pathway claims require Grade I-III evidence** with explicit uncertainty.
        3. **Driver identification requires Grade II-V evidence** with corroboration.
        4. **Intervention assessments may use Grade IV-V evidence** with clear flagging.
        5. **Grade VI material** provides context only and must be labeled.
        
        ## Reproducibility Requirements
        
        - All notebooks must run end-to-end from pinned environment
        - Raw data never hand-edited; all transformations via code
        - Figures/tables generated by documented scripts
        - Uncertainty ranges explicit on all quantitative claims
        
        ## Voice & Tone
        
        - Clinical precision with controlled fury
        - Specificity over abstraction
        - Show the math, skip the sermon
        - Name names where public record supports it
        - Acknowledge uncertainty like adults
        
        ---
        
        **Filed under: Causes of Death, Preventable**
        """)


def get_dossier_template(mechanism: dict[str, Any], vector_code: str) -> str:
    """Generate a DOSSIER.md template for a specific mechanism."""
    vector_name = VECTOR_FULL_NAMES.get(vector_code, "Unknown")
    stars = "‚òÖ" * mechanism.get("confidence", 3) + "‚òÜ" * (5 - mechanism.get("confidence", 3))
    
    return textwrap.dedent(f"""\
        # MECHANISM DOSSIER: {mechanism['code']}
        
        ## {mechanism['name']}
        
        **Vector:** {vector_name}  
        **Status:** {mechanism.get('status', 'Active')}  
        **Confidence:** {stars}  
        **Last Updated:** {datetime.now().strftime('%Y-%m-%d')}  
        **Lead Investigator:** [Unassigned]
        
        ---
        
        ## EXECUTIVE SUMMARY
        
        [3-5 sentences. What this mechanism does. Scale of damage. Key beneficiaries. 
        Why it persists. What would stop it.]
        
        ---
        
        ## THE DAMAGE
        
        ### What's Being Destroyed
        
        [Specific, quantified impacts. Not "biodiversity loss" but "X species extinct, 
        Y populations declined Z% since [year], habitat reduced by W hectares."]
        
        ### Who Bears the Cost
        
        [Human populations affected, with demographics. Ecosystems, species. Future 
        generations. Quantify where possible.]
        
        ### Geographic Footprint
        
        [Where this happens. Maps. Hotspots. Jurisdictional complexity.]
        
        ### Temporal Pattern
        
        [When did this start? Acceleration curve? Projections?]
        
        ---
        
        ## THE PATHWAY
        
        ### Causal Chain
        
        [Step-by-step: Input ‚Üí Process ‚Üí Output ‚Üí Impact. Be specific.]
        
        ```
        [DRIVER] ‚Üí [ACTIVITY] ‚Üí [EMISSION/EXTRACTION] ‚Üí [PATHWAY] ‚Üí [EXPOSURE] ‚Üí [DAMAGE]
        ```
        
        ### Feedback Loops
        
        [Self-reinforcing dynamics. What makes this worse over time?]
        
        ### Enabling Conditions
        
        [What allows this to continue? Legal frameworks, subsidies, information gaps, 
        collective action problems.]
        
        ---
        
        ## THE DRIVERS
        
        ### Who Benefits
        
        [Name specific companies, industries, investor classes. Quantify profits where 
        possible. Trace money flows.]
        
        ### Who Decides
        
        [Executives, board members, agency heads, elected officials. Name names where 
        public record supports it.]
        
        ### Political Economy
        
        [Lobbying expenditures. Campaign contributions. Revolving doors. Trade 
        associations. Think tank funding.]
        
        ---
        
        ## THE EVIDENCE
        
        ### Data Sources
        
        | Dataset | Source | Coverage | Quality | Access |
        |---------|--------|----------|---------|--------|
        | [Name] | [Provider] | [Years] | ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ | [link] |
        
        ### Key Findings
        
        1. **[Finding]** ‚Äî Evidence: [Tier], Sources: [refs]
        2. **[Finding]** ‚Äî Evidence: [Tier], Sources: [refs]
        
        ### Visualizations
        
        [Embed key figures with full provenance]
        
        ---
        
        ## THE UNCERTAINTY
        
        ### What We Don't Know
        
        [Explicit knowledge gaps]
        
        ### Measurement Limitations
        
        [Data quality issues, coverage gaps, definitional problems]
        
        ### Sensitivity Analysis
        
        [What assumptions matter most? What would change our conclusions?]
        
        ### Alternative Interpretations
        
        [Steel-man counterarguments. What would someone defending this mechanism say?]
        
        ---
        
        ## THE INTERVENTION QUESTION
        
        ### Leverage Points
        
        [Where could intervention actually work?]
        
        ### Resistance Factors
        
        [Why hasn't it been stopped? What interests oppose change?]
        
        ### Precedents
        
        [What's been tried? What happened?]
        
        ### What Would Actually Work
        
        [Be specific. Policy mechanisms, economic instruments, enforcement requirements.]
        
        ### Difficulty Score: [X/10]
        
        [Justify the rating]
        
        ---
        
        ## CONNECTIONS
        
        ### Upstream
        
        [Mechanisms that feed into this one]
        
        - [CODE]: [Mechanism Name] ‚Äî [relationship]
        
        ### Downstream
        
        [Mechanisms this one feeds into]
        
        - [CODE]: [Mechanism Name] ‚Äî [relationship]
        
        ### Cross-Vector Links
        
        [Connections across damage vectors]
        
        - [CODE]: [Mechanism Name] ‚Äî [relationship]
        
        ---
        
        ## EXHIBITS
        
        | ID | Type | Description | File |
        |----|------|-------------|------|
        | E01 | Figure | [Description] | `exhibits/figures/fig_01.png` |
        
        ---
        
        **Filed under: Causes of Death, Preventable**
        
        ---
        
        *Last revision: {datetime.now().strftime('%Y-%m-%d')} | Version: 0.1.0*
        """)


def get_notebook_intake_template(mechanism: dict[str, Any]) -> str:
    """Generate 01_intake.ipynb content as Python script (convert later if needed)."""
    return textwrap.dedent(f'''\
        # ---
        # jupyter:
        #   jupytext:
        #     text_representation:
        #       extension: .py
        #       format_name: percent
        #   kernelspec:
        #     display_name: Python 3
        #     language: python
        #     name: python3
        # ---

        # %% [markdown]
        # # Data Intake: {mechanism['code']} - {mechanism['name']}
        # 
        # **Purpose:** Download, validate, and clean source datasets for this mechanism.
        # 
        # **Outputs:**
        # - `../datasets/processed/clean_dataset.parquet`
        # - `../datasets/derived/data_dictionary.csv`

        # %% [markdown]
        # ## Setup

        # %%
        import pandas as pd
        from pathlib import Path
        from datetime import datetime

        # Configuration
        MECHANISM_CODE = "{mechanism['code']}"
        RAW_DATA_DIR = Path("../datasets/raw")
        PROCESSED_DIR = Path("../datasets/processed")
        DERIVED_DIR = Path("../datasets/derived")

        # Ensure directories exist
        RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
        PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
        DERIVED_DIR.mkdir(parents=True, exist_ok=True)

        # %% [markdown]
        # ## 1. Data Acquisition
        # 
        # Document all data sources and download methods here.

        # %%
        # TODO: Implement data download
        # Example:
        # import requests
        # url = "https://example.com/data.csv"
        # response = requests.get(url)
        # (RAW_DATA_DIR / "source_data.csv").write_bytes(response.content)

        print(f"[{{datetime.now()}}] Data acquisition step - implement for {mechanism['code']}")

        # %% [markdown]
        # ## 2. Schema Validation
        # 
        # Verify expected columns, types, and constraints.

        # %%
        # TODO: Load and validate schema
        # df = pd.read_csv(RAW_DATA_DIR / "source_data.csv")
        # 
        # Expected schema:
        # EXPECTED_COLUMNS = ["year", "value", "unit", "source"]
        # assert all(col in df.columns for col in EXPECTED_COLUMNS), "Missing columns"

        print("Schema validation step - implement validation logic")

        # %% [markdown]
        # ## 3. Data Cleaning
        # 
        # Handle missing values, outliers, unit conversions.

        # %%
        # TODO: Implement cleaning pipeline
        # df_clean = df.copy()
        # df_clean = df_clean.dropna(subset=["value"])
        # df_clean["value"] = pd.to_numeric(df_clean["value"], errors="coerce")

        print("Data cleaning step - implement cleaning logic")

        # %% [markdown]
        # ## 4. Export Processed Data

        # %%
        # TODO: Save cleaned data
        # df_clean.to_parquet(PROCESSED_DIR / "clean_dataset.parquet", index=False)
        # 
        # # Generate data dictionary
        # data_dict = pd.DataFrame({{
        #     "column": df_clean.columns,
        #     "dtype": df_clean.dtypes.astype(str),
        #     "non_null_count": df_clean.count(),
        #     "description": ["TODO"] * len(df_clean.columns)
        # }})
        # data_dict.to_csv(DERIVED_DIR / "data_dictionary.csv", index=False)

        print(f"[{{datetime.now()}}] Intake complete for {mechanism['code']}")

        # %% [markdown]
        # ## Provenance Log
        # 
        # | Action | Timestamp | Notes |
        # |--------|-----------|-------|
        # | Created | {datetime.now().strftime('%Y-%m-%d')} | Initial template |
        ''')


def get_notebook_analysis_template(mechanism: dict[str, Any]) -> str:
    """Generate 02_analysis.ipynb content."""
    return textwrap.dedent(f'''\
        # ---
        # jupyter:
        #   jupytext:
        #     text_representation:
        #       extension: .py
        #       format_name: percent
        #   kernelspec:
        #     display_name: Python 3
        #     language: python
        #     name: python3
        # ---

        # %% [markdown]
        # # Analysis: {mechanism['code']} - {mechanism['name']}
        # 
        # **Purpose:** Compute derived metrics, run scenarios, sensitivity analysis.
        # 
        # **Inputs:**
        # - `../datasets/processed/clean_dataset.parquet`
        # 
        # **Outputs:**
        # - `../datasets/derived/results.csv`
        # - `../datasets/derived/parameter_sweeps.csv`

        # %% [markdown]
        # ## Setup

        # %%
        import pandas as pd
        import numpy as np
        from pathlib import Path

        MECHANISM_CODE = "{mechanism['code']}"
        PROCESSED_DIR = Path("../datasets/processed")
        DERIVED_DIR = Path("../datasets/derived")

        # %% [markdown]
        # ## 1. Load Processed Data

        # %%
        # TODO: Load data
        # df = pd.read_parquet(PROCESSED_DIR / "clean_dataset.parquet")
        # print(f"Loaded {{len(df)}} records")

        print("Load step - implement after intake notebook complete")

        # %% [markdown]
        # ## 2. Derived Metrics
        # 
        # Calculate per-capita, intensity curves, growth rates.

        # %%
        # TODO: Implement metric calculations
        # df["per_capita"] = df["value"] / df["population"]
        # df["yoy_change"] = df.groupby("region")["value"].pct_change()

        print("Metrics step - implement derived calculations")

        # %% [markdown]
        # ## 3. Scenario Analysis
        # 
        # Model different assumptions about future trajectories.

        # %%
        # TODO: Implement scenarios
        # scenarios = {{
        #     "baseline": {{"growth_rate": 0.02}},
        #     "accelerated": {{"growth_rate": 0.05}},
        #     "mitigation": {{"growth_rate": -0.01}},
        # }}

        print("Scenario step - implement projections")

        # %% [markdown]
        # ## 4. Sensitivity Tests
        # 
        # Which parameters most affect conclusions?

        # %%
        # TODO: Parameter sweeps
        # param_range = np.linspace(0.01, 0.10, 10)
        # results = []
        # for param in param_range:
        #     result = model(param)
        #     results.append({{"param": param, "outcome": result}})
        # sensitivity_df = pd.DataFrame(results)

        print("Sensitivity step - implement parameter sweeps")

        # %% [markdown]
        # ## 5. Export Results

        # %%
        # TODO: Save outputs
        # results_df.to_csv(DERIVED_DIR / "results.csv", index=False)
        # sensitivity_df.to_csv(DERIVED_DIR / "parameter_sweeps.csv", index=False)

        print(f"Analysis complete for {mechanism['code']}")
        ''')


def get_notebook_outputs_template(mechanism: dict[str, Any]) -> str:
    """Generate 03_outputs.ipynb content."""
    return textwrap.dedent(f'''\
        # ---
        # jupyter:
        #   jupytext:
        #     text_representation:
        #       extension: .py
        #       format_name: percent
        #   kernelspec:
        #     display_name: Python 3
        #     language: python
        #     name: python3
        # ---

        # %% [markdown]
        # # Outputs: {mechanism['code']} - {mechanism['name']}
        # 
        # **Purpose:** Generate all figures, tables, and maps for the dossier.
        # 
        # **Inputs:**
        # - `../datasets/derived/results.csv`
        # 
        # **Outputs:**
        # - `../../exhibits/figures/*.png`
        # - `../../exhibits/tables/*.csv`

        # %% [markdown]
        # ## Setup

        # %%
        import pandas as pd
        import matplotlib.pyplot as plt
        from pathlib import Path

        MECHANISM_CODE = "{mechanism['code']}"
        DERIVED_DIR = Path("../datasets/derived")
        FIGURES_DIR = Path("../../exhibits/figures")
        TABLES_DIR = Path("../../exhibits/tables")

        FIGURES_DIR.mkdir(parents=True, exist_ok=True)
        TABLES_DIR.mkdir(parents=True, exist_ok=True)

        # Style configuration
        plt.style.use("seaborn-v0_8-whitegrid")
        COLORS = {{
            "primary": "#2e3c44",
            "secondary": "#ff9e54",
            "tertiary": "#46ff84",
            "accent": "#f5f5f4",
            "highlight": "#ffd144",
            "success": "#44cca4",
        }}

        # %% [markdown]
        # ## 1. Load Analysis Results

        # %%
        # TODO: Load data
        # results = pd.read_csv(DERIVED_DIR / "results.csv")

        print("Load analysis results")

        # %% [markdown]
        # ## 2. Figure: Main Trend

        # %%
        # TODO: Create primary figure
        # fig, ax = plt.subplots(figsize=(10, 6))
        # ax.plot(results["year"], results["value"], color=COLORS["alert"], lw=2)
        # ax.set_title(f"{mechanism['name']}: Primary Trend")
        # ax.set_xlabel("Year")
        # ax.set_ylabel("Value")
        # fig.savefig(FIGURES_DIR / "fig_01_trend.png", dpi=150, bbox_inches="tight")
        # plt.close()

        print("Figure generation step")

        # %% [markdown]
        # ## 3. Figure: Geographic Distribution

        # %%
        # TODO: Create map visualization
        print("Map generation step - implement if geographic data available")

        # %% [markdown]
        # ## 4. Summary Tables

        # %%
        # TODO: Export formatted tables
        # summary = results.groupby("category").agg({{"value": ["mean", "std", "count"]}})
        # summary.to_csv(TABLES_DIR / "table_01_summary.csv")

        print("Table generation step")

        # %% [markdown]
        # ## 5. Output Manifest
        # 
        # List all generated artifacts:

        # %%
        print("Generated outputs:")
        for f in FIGURES_DIR.glob("*"):
            print(f"  - {{f}}")
        for t in TABLES_DIR.glob("*"):
            print(f"  - {{t}}")

        # %% [markdown]
        # ---
        # 
        # **Filed under: Causes of Death, Preventable**
        ''')


def get_environment_template() -> str:
    """Generate environment.yml for conda/mamba."""
    return textwrap.dedent("""\
        # Terminal Velocity Computational Environment
        # Install: conda env create -f environment.yml
        # Activate: conda activate tv-research
        
        name: tv-research
        channels:
          - conda-forge
          - defaults
        
        dependencies:
          - python=3.11
          - pandas>=2.0
          - numpy>=1.24
          - matplotlib>=3.7
          - seaborn>=0.12
          - scipy>=1.10
          - scikit-learn>=1.2
          - pyarrow>=12.0
          - openpyxl>=3.1
          - xlsxwriter>=3.1
          - requests>=2.28
          - beautifulsoup4>=4.12
          - lxml>=4.9
          - pyyaml>=6.0
          - jupyter>=1.0
          - jupyterlab>=4.0
          - jupytext>=1.14
          - black>=23.0
          - ruff>=0.1
          - pytest>=7.3
          - pip
          - pip:
            - rich>=13.0
            - geopandas>=0.13
            - folium>=0.14
        """)


def get_gitignore_template() -> str:
    """Generate .gitignore for the project."""
    return textwrap.dedent("""\
        # Terminal Velocity .gitignore
        
        # Python
        __pycache__/
        *.py[cod]
        *$py.class
        *.so
        .Python
        build/
        develop-eggs/
        dist/
        downloads/
        eggs/
        .eggs/
        lib/
        lib64/
        parts/
        sdist/
        var/
        wheels/
        *.egg-info/
        .installed.cfg
        *.egg
        
        # Virtual environments
        .env
        .venv
        env/
        venv/
        ENV/
        
        # Jupyter
        .ipynb_checkpoints/
        *.ipynb_checkpoints
        
        # IDE
        .idea/
        .vscode/
        *.swp
        *.swo
        *~
        
        # OS
        .DS_Store
        .DS_Store?
        ._*
        .Spotlight-V100
        .Trashes
        ehthumbs.db
        Thumbs.db
        
        # Data (raw data should be downloaded, not committed)
        **/datasets/raw/*
        !**/datasets/raw/.gitkeep
        
        # Large files
        *.csv
        *.parquet
        *.xlsx
        *.xls
        !**/exhibits/tables/*.csv
        
        # Secrets
        .env
        .env.local
        secrets.yaml
        credentials.json
        
        # Logs
        *.log
        logs/
        
        # Temp
        tmp/
        temp/
        *.tmp
        """)


def get_gitkeep_placeholder() -> str:
    """Generate .gitkeep content."""
    return "# Placeholder to preserve empty directory in git\n"


# =============================================================================
# DIRECTORY STRUCTURE DEFINITION
# =============================================================================

def get_infrastructure_structure() -> dict:
    """Define the shared infrastructure directory structure."""
    return {
        "README.md": get_readme_template(),
        "MANIFEST.md": None,  # Generated dynamically
        "METHODOLOGY.md": get_methodology_template(),
        "connection_atlas": {
            "mechanism_network.md": "# Mechanism Network\n\n[Connection map to be generated]\n",
            "causal_chains": {".gitkeep": get_gitkeep_placeholder()},
            "feedback_systems": {".gitkeep": get_gitkeep_placeholder()},
            "visualizations": {".gitkeep": get_gitkeep_placeholder()},
        },
        "evidence_vault": {
            "dataset_registry.md": "# Dataset Registry\n\n| ID | Name | Source | Coverage | Quality | Link |\n|---|---|---|---|---|---|\n",
            "source_reliability_index.md": "# Source Reliability Index\n\n[Quality ratings for all sources]\n",
            "raw_data": {".gitkeep": get_gitkeep_placeholder()},
            "reference_documents": {".gitkeep": get_gitkeep_placeholder()},
            "download_scripts": {".gitkeep": get_gitkeep_placeholder()},
        },
        "synthesis_products": {
            "executive_briefs": {".gitkeep": get_gitkeep_placeholder()},
            "vector_reports": {".gitkeep": get_gitkeep_placeholder()},
            "connection_analyses": {".gitkeep": get_gitkeep_placeholder()},
            "intervention_assessments": {".gitkeep": get_gitkeep_placeholder()},
        },
        "computational_core": {
            "shared_utils": {
                "__init__.py": '"""Terminal Velocity shared utilities."""\n',
                "data_utils.py": '"""Data loading and transformation utilities."""\n\n# TODO: Implement shared functions\n',
                "viz_utils.py": '"""Visualization utilities."""\n\n# TODO: Implement shared plotting functions\n',
            },
            "environment_specs": {
                "environment.yml": get_environment_template(),
                "requirements.txt": "# Pip requirements (generated from environment.yml)\npandas>=2.0\nnumpy>=1.24\nmatplotlib>=3.7\nrich>=13.0\n",
            },
            "validation_suite": {".gitkeep": get_gitkeep_placeholder()},
            "output_templates": {".gitkeep": get_gitkeep_placeholder()},
        },
        "visual_assets": {
            "templates": {".gitkeep": get_gitkeep_placeholder()},
            "iconography": {".gitkeep": get_gitkeep_placeholder()},
            "color_schemes": {
                "tv_colors.yaml": textwrap.dedent("""\
                    # Terminal Velocity Color Palette
                    # Note: All colors end in "4" ‚Äî the harmonic signature
                    primary: "#2e3c44"      # Deep Slate - Background, authority
                    secondary: "#ff9e54"    # Burnt Orange - Damage, urgency
                    tertiary: "#46ff84"     # Electric Green - Data, evidence
                    accent: "#f5f5f4"       # Soft White - Text, contrast
                    highlight: "#ffd144"    # Gold - Uncertainty, caution
                    success: "#44cca4"      # Teal - Solutions, leverage points
                    """),
            },
            "brand_guide.md": "# Terminal Velocity Visual Identity\n\n[Brand guidelines to be documented]\n",
        },
        "meta": {
            "changelog.md": f"# Changelog\n\n## [0.1.0] - {datetime.now().strftime('%Y-%m-%d')}\n\n### Added\n- Initial project scaffold\n",
            "contributors.md": "# Contributors\n\n- [Lead] - Project creator\n",
            "license.md": "# License\n\n[Specify license terms]\n",
            "citation.md": "# Citation\n\n[How to cite this work]\n",
        },
        ".gitignore": get_gitignore_template(),
        ".gitattributes": "*.ipynb filter=nbstripout\n*.parquet filter=lfs diff=lfs merge=lfs -text\n",
    }


def get_mechanism_structure(mechanism: dict[str, Any], vector_code: str) -> dict:
    """Generate the directory structure for a single mechanism dossier."""
    return {
        "DOSSIER.md": get_dossier_template(mechanism, vector_code),
        "damage_report": {
            "impact_inventory.md": f"# Impact Inventory: {mechanism['code']}\n\n[Quantified damage assessment]\n",
            "victim_profile.md": f"# Victim Profile: {mechanism['code']}\n\n[Who/what bears the cost]\n",
            "geographic_scope.md": f"# Geographic Scope: {mechanism['code']}\n\n[Spatial distribution of impacts]\n",
            "temporal_pattern.md": f"# Temporal Pattern: {mechanism['code']}\n\n[Historical trends and projections]\n",
        },
        "pathway_analysis": {
            "causal_chain.md": f"# Causal Chain: {mechanism['code']}\n\n[Step-by-step mechanism]\n",
            "feedback_loops.md": f"# Feedback Loops: {mechanism['code']}\n\n[Self-reinforcing dynamics]\n",
            "enabling_conditions.md": f"# Enabling Conditions: {mechanism['code']}\n\n[What allows continuation]\n",
            "physical_flows.md": f"# Physical Flows: {mechanism['code']}\n\n[Material/energy accounting]\n",
        },
        "driver_profile": {
            "beneficiary_map.md": f"# Beneficiary Map: {mechanism['code']}\n\n[Who profits, how much]\n",
            "decision_points.md": f"# Decision Points: {mechanism['code']}\n\n[Where choices are made]\n",
            "political_economy.md": f"# Political Economy: {mechanism['code']}\n\n[Power structures, lobbying]\n",
            "named_actors.md": f"# Named Actors: {mechanism['code']}\n\n[Specific entities - PUBLIC RECORD ONLY]\n",
        },
        "evidence_archive": {
            "datasets": {
                "raw": {".gitkeep": get_gitkeep_placeholder()},
                "processed": {".gitkeep": get_gitkeep_placeholder()},
                "derived": {".gitkeep": get_gitkeep_placeholder()},
            },
            "sources": {
                "bibliography.bib": f"% Bibliography for {mechanism['code']}\n% Add BibTeX entries below\n",
                "source_reliability.md": f"# Source Reliability: {mechanism['code']}\n\n| Source | Grade | Notes |\n|--------|-------|-------|\n",
                "claim_trace.md": f"# Claim Traceability: {mechanism['code']}\n\n| Claim ID | Claim | Source | Evidence Grade |\n|----------|-------|--------|----------------|\n",
            },
            "computations": {
                "01_intake.py": get_notebook_intake_template(mechanism),
                "02_analysis.py": get_notebook_analysis_template(mechanism),
                "03_outputs.py": get_notebook_outputs_template(mechanism),
                "environment.yml": get_environment_template(),
                "README.md": f"# Computations: {mechanism['code']}\n\n## Running the Pipeline\n\n```bash\ncd computations\nconda env create -f environment.yml\nconda activate tv-research\npython 01_intake.py\npython 02_analysis.py\npython 03_outputs.py\n```\n\n## Outputs\n\n- `../datasets/processed/` - Cleaned data\n- `../datasets/derived/` - Computed metrics\n- `../../exhibits/` - Figures and tables\n",
            },
        },
        "uncertainty_register": {
            "known_unknowns.md": f"# Known Unknowns: {mechanism['code']}\n\n[Explicit knowledge gaps]\n",
            "measurement_limits.md": f"# Measurement Limits: {mechanism['code']}\n\n[Data quality issues]\n",
            "model_assumptions.md": f"# Model Assumptions: {mechanism['code']}\n\n[Where we're estimating]\n",
            "sensitivity_tests.md": f"# Sensitivity Tests: {mechanism['code']}\n\n[Parameter importance]\n",
            "alternative_interpretations.md": f"# Alternative Interpretations: {mechanism['code']}\n\n[Steel-man counterarguments]\n",
        },
        "intervention_assessment": {
            "leverage_points.md": f"# Leverage Points: {mechanism['code']}\n\n[Where intervention could work]\n",
            "resistance_factors.md": f"# Resistance Factors: {mechanism['code']}\n\n[Why it persists]\n",
            "precedents.md": f"# Precedents: {mechanism['code']}\n\n[What's been tried]\n",
            "requirements.md": f"# Requirements: {mechanism['code']}\n\n[What would actually work]\n",
            "difficulty_score.md": f"# Difficulty Score: {mechanism['code']}\n\n**Score:** [X/10]\n\n**Justification:**\n\n[Explain rating]\n",
        },
        "connection_map": {
            "upstream_mechanisms.md": f"# Upstream: {mechanism['code']}\n\n[Mechanisms that feed into this]\n",
            "downstream_mechanisms.md": f"# Downstream: {mechanism['code']}\n\n[Mechanisms this feeds into]\n",
            "parallel_mechanisms.md": f"# Parallel: {mechanism['code']}\n\n[Related patterns]\n",
            "cross_vector_links.md": f"# Cross-Vector: {mechanism['code']}\n\n[Connections to other damage vectors]\n",
        },
        "exhibits": {
            "figures": {".gitkeep": get_gitkeep_placeholder()},
            "tables": {".gitkeep": get_gitkeep_placeholder()},
            "diagrams": {".gitkeep": get_gitkeep_placeholder()},
            "maps": {".gitkeep": get_gitkeep_placeholder()},
            "timeline": {".gitkeep": get_gitkeep_placeholder()},
        },
    }


# =============================================================================
# BUILDER CLASS
# =============================================================================

class TerminalVelocityScaffold:
    """Builder class for Terminal Velocity project structure."""
    
    def __init__(
        self,
        target_dir: Path,
        vectors: list[str] | None = None,
        mechanisms: dict[str, list] | None = None,
        dry_run: bool = False,
        force: bool = False,
        verbose: bool = False,
        quiet: bool = False,
        init_git: bool = True,
    ):
        self.target_dir = Path(target_dir).resolve()
        self.vectors = vectors or list(VECTOR_NAMES.keys())
        self.mechanisms = mechanisms or {v: DEFAULT_MECHANISMS.get(v, []) for v in self.vectors}
        self.dry_run = dry_run
        self.force = force
        self.verbose = verbose
        self.quiet = quiet
        self.init_git = init_git
        
        self.files_created: list[Path] = []
        self.dirs_created: list[Path] = []
        self.files_skipped: list[Path] = []
        
        # Console for rich output
        self.console = Console() if RICH_AVAILABLE else None
    
    def log(self, message: str, level: str = "info") -> None:
        """Log a message based on verbosity settings."""
        if self.quiet and level != "error":
            return
        if not self.verbose and level == "debug":
            return
        
        if RICH_AVAILABLE and self.console:
            styles = {
                "info": "cyan",
                "success": "green",
                "warning": "yellow",
                "error": "red bold",
                "debug": "dim",
            }
            self.console.print(f"[{styles.get(level, 'white')}]{message}[/]")
        else:
            print(message)
    
    def create_directory(self, path: Path) -> bool:
        """Create a directory if it doesn't exist."""
        if self.dry_run:
            self.dirs_created.append(path)
            return True
        
        try:
            path.mkdir(parents=True, exist_ok=True)
            self.dirs_created.append(path)
            return True
        except PermissionError:
            self.log(f"Permission denied: {path}", "error")
            return False
    
    def create_file(self, path: Path, content: str) -> bool:
        """Create a file with the given content."""
        if path.exists() and not self.force:
            self.files_skipped.append(path)
            self.log(f"Skipped (exists): {path}", "debug")
            return False
        
        if self.dry_run:
            self.files_created.append(path)
            return True
        
        try:
            self.create_directory(path.parent)
            path.write_text(content, encoding="utf-8")
            self.files_created.append(path)
            self.log(f"Created: {path.relative_to(self.target_dir)}", "debug")
            return True
        except PermissionError:
            self.log(f"Permission denied: {path}", "error")
            return False
    
    def build_from_structure(self, structure: dict, base_path: Path) -> None:
        """Recursively build directory structure from a dict definition."""
        for name, content in structure.items():
            path = base_path / name
            
            if isinstance(content, dict):
                self.create_directory(path)
                self.build_from_structure(content, path)
            elif isinstance(content, str):
                self.create_file(path, content)
            elif content is None:
                pass  # Skip, will be generated separately
    
    def build_infrastructure(self) -> None:
        """Build the shared infrastructure directories."""
        self.log("Building infrastructure...", "info")
        structure = get_infrastructure_structure()
        self.build_from_structure(structure, self.target_dir)
    
    def build_mechanisms(self) -> None:
        """Build all mechanism dossier directories."""
        self.log("Building mechanism dossiers...", "info")
        
        dossiers_dir = self.target_dir / "mechanism_dossiers"
        self.create_directory(dossiers_dir)
        
        total_mechanisms = sum(len(mechs) for mechs in self.mechanisms.values())
        current = 0
        
        for vector_code in sorted(self.mechanisms.keys()):
            vector_name = VECTOR_NAMES.get(vector_code, vector_code.lower())
            vector_dir = dossiers_dir / f"{vector_code}_{vector_name}"
            self.create_directory(vector_dir)
            
            for mechanism in self.mechanisms[vector_code]:
                current += 1
                mech_dir_name = f"{mechanism['code']}_{mechanism['slug']}"
                mech_dir = vector_dir / mech_dir_name
                
                self.log(f"  [{current}/{total_mechanisms}] {mechanism['code']}: {mechanism['name']}", "debug")
                
                structure = get_mechanism_structure(mechanism, vector_code)
                self.create_directory(mech_dir)
                self.build_from_structure(structure, mech_dir)
    
    def generate_manifest(self) -> None:
        """Generate the MANIFEST.md file."""
        manifest_content = get_manifest_template(self.mechanisms)
        self.create_file(self.target_dir / "MANIFEST.md", manifest_content)
    
    def initialize_git(self) -> None:
        """Initialize git repository if requested."""
        if not self.init_git or self.dry_run:
            return
        
        import subprocess
        
        git_dir = self.target_dir / ".git"
        if git_dir.exists():
            self.log("Git repository already exists", "debug")
            return
        
        try:
            subprocess.run(
                ["git", "init"],
                cwd=self.target_dir,
                capture_output=True,
                check=True,
            )
            self.log("Initialized git repository", "success")
        except (subprocess.CalledProcessError, FileNotFoundError):
            self.log("Git initialization failed (git not available?)", "warning")
    
    def print_tree(self) -> None:
        """Print the directory tree (for dry-run visualization)."""
        if not RICH_AVAILABLE:
            self.log("Install 'rich' package for tree visualization", "warning")
            return
        
        tree = Tree(f"üìÅ {self.target_dir.name}")
        
        def add_to_tree(parent: Tree, path: Path, depth: int = 0) -> None:
            if depth > 3:  # Limit depth for readability
                return
            
            try:
                items = sorted(path.iterdir()) if path.exists() else []
            except PermissionError:
                return
            
            dirs = [p for p in items if p.is_dir() and not p.name.startswith(".")]
            files = [p for p in items if p.is_file() and not p.name.startswith(".")]
            
            for d in dirs[:10]:  # Limit shown directories
                subtree = parent.add(f"üìÅ {d.name}")
                add_to_tree(subtree, d, depth + 1)
            
            if len(dirs) > 10:
                parent.add(f"[dim]... and {len(dirs) - 10} more directories[/]")
            
            for f in files[:5]:  # Limit shown files
                parent.add(f"üìÑ {f.name}")
            
            if len(files) > 5:
                parent.add(f"[dim]... and {len(files) - 5} more files[/]")
        
        # For dry run, just show structure summary
        if self.dry_run:
            tree.add(f"üìÅ mechanism_dossiers/ ({len(self.vectors)} vectors)")
            for v in sorted(self.vectors):
                count = len(self.mechanisms.get(v, []))
                tree.add(f"  üìÅ {v}_{VECTOR_NAMES.get(v, 'unknown')}/ ({count} mechanisms)")
            tree.add("üìÅ connection_atlas/")
            tree.add("üìÅ evidence_vault/")
            tree.add("üìÅ synthesis_products/")
            tree.add("üìÅ computational_core/")
            tree.add("üìÅ visual_assets/")
            tree.add("üìÅ meta/")
            tree.add("üìÑ README.md")
            tree.add("üìÑ MANIFEST.md")
            tree.add("üìÑ METHODOLOGY.md")
        else:
            add_to_tree(tree, self.target_dir)
        
        self.console.print(tree)
    
    def print_summary(self) -> None:
        """Print build summary."""
        if RICH_AVAILABLE and self.console:
            summary = f"""
[bold cyan]Terminal Velocity Scaffold Complete[/]

[green]‚úì[/] Directories created: {len(self.dirs_created)}
[green]‚úì[/] Files created: {len(self.files_created)}
[yellow]‚óã[/] Files skipped: {len(self.files_skipped)}

[bold]Vectors built:[/] {', '.join(sorted(self.vectors))}
[bold]Total mechanisms:[/] {sum(len(m) for m in self.mechanisms.values())}

[dim]Target: {self.target_dir}[/]
"""
            self.console.print(Panel(summary, title="Build Summary", border_style="green"))
        else:
            print(f"\nBuild complete: {len(self.dirs_created)} dirs, {len(self.files_created)} files")
            print(f"Target: {self.target_dir}")
    
    def build(self) -> int:
        """Execute the full build process."""
        # Validate
        if self.target_dir.exists() and not self.force and not self.dry_run:
            if any(self.target_dir.iterdir()):
                self.log(f"Target directory not empty: {self.target_dir}", "error")
                self.log("Use --force to overwrite or choose a different directory", "error")
                return 2
        
        # Header
        if not self.quiet:
            if RICH_AVAILABLE and self.console:
                self.console.print(Panel.fit(
                    f"[bold red]TERMINAL VELOCITY[/]\n[dim]Forensic Research Framework Scaffold[/]",
                    border_style="red",
                ))
            else:
                print("\n=== TERMINAL VELOCITY SCAFFOLD ===\n")
        
        if self.dry_run:
            self.log("DRY RUN - No files will be created", "warning")
        
        # Build
        self.create_directory(self.target_dir)
        self.build_infrastructure()
        self.build_mechanisms()
        self.generate_manifest()
        self.initialize_git()
        
        # Summary
        if not self.quiet:
            if self.dry_run:
                self.print_tree()
            self.print_summary()
        
        return 0


# =============================================================================
# CLI INTERFACE
# =============================================================================

def parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        prog=PROGRAM_NAME,
        description="Build Terminal Velocity research framework directory structure",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Examples:
  %(prog)s ~/research/terminal-velocity
  %(prog)s ./my-project --dry-run
  %(prog)s ./my-project --vectors A B C
  %(prog)s ./my-project --verbose

{SIGNATURE}
        """,
    )
    
    parser.add_argument(
        "target_dir",
        type=Path,
        help="Root directory for the project (will be created)",
    )
    
    # Vector selection
    vector_group = parser.add_argument_group("Vector Selection")
    vector_group.add_argument(
        "--vectors", "-V",
        nargs="+",
        choices=list(VECTOR_NAMES.keys()),
        default=None,
        help="Space-separated list of vectors to build (default: all)",
    )
    vector_group.add_argument(
        "--infrastructure-only",
        action="store_true",
        help="Build only shared infrastructure, no dossiers",
    )
    
    # Mechanism options
    mech_group = parser.add_argument_group("Mechanism Options")
    mech_group.add_argument(
        "--mechanisms",
        type=Path,
        metavar="FILE",
        help="YAML file with custom mechanism definitions",
    )
    mech_group.add_argument(
        "--skip-defaults",
        action="store_true",
        help="Don't include default mechanisms (use with --mechanisms)",
    )
    
    # Output control
    output_group = parser.add_argument_group("Output Control")
    output_group.add_argument(
        "--dry-run", "-n",
        action="store_true",
        help="Preview directory tree without creating anything",
    )
    output_group.add_argument(
        "--force", "-f",
        action="store_true",
        help="Overwrite existing files",
    )
    output_group.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Detailed progress output",
    )
    output_group.add_argument(
        "--quiet", "-q",
        action="store_true",
        help="Suppress all output except errors",
    )
    
    # Git options
    git_group = parser.add_argument_group("Git Options")
    git_group.add_argument(
        "--no-git",
        action="store_true",
        help="Skip git repository initialization",
    )
    
    # Version
    parser.add_argument(
        "--version",
        action="version",
        version=f"%(prog)s {VERSION}",
    )
    
    return parser.parse_args()


def load_custom_mechanisms(filepath: Path) -> dict[str, list]:
    """Load custom mechanisms from YAML file."""
    if not YAML_AVAILABLE:
        print("Error: PyYAML required for custom mechanisms. Install with: pip install pyyaml")
        sys.exit(4)
    
    if not filepath.exists():
        print(f"Error: Mechanism file not found: {filepath}")
        sys.exit(4)
    
    try:
        with open(filepath, encoding="utf-8") as f:
            data = yaml.safe_load(f)
        
        # Validate structure
        if not isinstance(data, dict):
            raise ValueError("Root must be a dictionary of vectors")
        
        mechanisms = {}
        for vector, mechs in data.items():
            if vector not in VECTOR_NAMES:
                print(f"Warning: Unknown vector '{vector}', skipping")
                continue
            
            if not isinstance(mechs, list):
                raise ValueError(f"Vector '{vector}' must contain a list of mechanisms")
            
            mechanisms[vector] = []
            for m in mechs:
                required = {"code", "slug", "name"}
                if not required.issubset(m.keys()):
                    raise ValueError(f"Mechanism missing required fields: {required - set(m.keys())}")
                mechanisms[vector].append(m)
        
        return mechanisms
    
    except Exception as e:
        print(f"Error loading mechanism file: {e}")
        sys.exit(4)


def main() -> int:
    """Main entry point."""
    args = parse_args()
    
    # Handle custom mechanisms
    mechanisms = None
    if args.mechanisms:
        custom = load_custom_mechanisms(args.mechanisms)
        if args.skip_defaults:
            mechanisms = custom
        else:
            mechanisms = {**DEFAULT_MECHANISMS, **custom}
    
    # Handle infrastructure-only mode
    if args.infrastructure_only:
        mechanisms = {}
    
    # Build vectors list
    vectors = args.vectors
    if mechanisms is not None and vectors is None:
        vectors = list(mechanisms.keys())
    
    # Create and run builder
    builder = TerminalVelocityScaffold(
        target_dir=args.target_dir,
        vectors=vectors,
        mechanisms=mechanisms,
        dry_run=args.dry_run,
        force=args.force,
        verbose=args.verbose,
        quiet=args.quiet,
        init_git=not args.no_git,
    )
    
    return builder.build()


if __name__ == "__main__":
    sys.exit(main())
